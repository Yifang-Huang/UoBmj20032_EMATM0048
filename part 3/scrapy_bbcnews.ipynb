{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import feedparser\n",
    "import requests\n",
    "import datetime\n",
    "import lxml\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the file that includes the steps for us to scrapy the content from BBC News."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handle missing data\n",
    "BBC News has several different formats of the web page, so we will use different ways to access them. Particularly, some News only include the video/image content, without any text content. For this situation, we will use their article descriptions as content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to access the content of each article. We will pass the article URL addresses into the function, and it will return each full content of articles. This function is based on Beautiful Soup 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_content(url):\n",
    "    news_url = url\n",
    "    data = requests.get(news_url).text\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    " \n",
    " \n",
    "    article = ''\n",
    " \n",
    "  \n",
    "    extract = soup.find_all('div', {\"class\": \"css-uf6wea-RichTextComponentWrapper e1xue1i83\"})\n",
    "   \n",
    "    if extract != []:\n",
    "      \n",
    "        for content in extract:\n",
    "            article = article + ' ' + content.text\n",
    " \n",
    "        return article\n",
    " \n",
    "    else:\n",
    "        \n",
    "        try:\n",
    "            extract = soup.find('div', {\"class\": \"css-83cqas-RichTextContainer e5tfeyi2\"}).extract()\n",
    "            extracts = extract.find_all_next('p')\n",
    " \n",
    " \n",
    "            for content in extracts:\n",
    "                article = article + ' ' + content.text\n",
    " \n",
    "            return article\n",
    " \n",
    " \n",
    "        except:\n",
    "            article = soup.find('meta', attrs={'name': 'description'}).get('content')\n",
    "            return article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create a csv file with the heading: Title, Content, Tag, Time and URL. \n",
    "\n",
    "Then we call this function to create a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv():\n",
    "    rules = ['Title','Content','Tag','Time','URL']\n",
    "    f = open('./bbcnews.csv','a+',newline='',encoding='utf-8')\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(rules)\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to save the data into csv file.\n",
    "\n",
    "The data includes article title, article content, article tag, updated time and url address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(title,content,tag,time,url):\n",
    "    f = open('./bbcnews.csv','a+',newline='',encoding='utf-8')\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([title,content,tag,time,url])\n",
    " \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will crawl data from 9 BBC RSS feeds, including BBC-World, BBC-UK, BBC-politics..etc...\n",
    "\n",
    "My variables of interest are title and contents of articles which I can use statistical means to analyze the number of their words and analyze what are the most common words of the same type in articles.\n",
    "\n",
    "We can access URL address, updated time, article title, and tag directly from the feed. For the content of the article, we have to use the function which we defined before (request_content) to access. we will pass the article URL address to this function and it will return the full content of this article.\n",
    "Then we will use save_to_csv function to save the data into the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS_URLS=[\n",
    "        'http://feeds.bbci.co.uk/news/world/rss.xml',\n",
    "        'http://feeds.bbci.co.uk/news/uk/rss.xml',\n",
    "        'http://feeds.bbci.co.uk/news/business/rss.xml',\n",
    "        'http://feeds.bbci.co.uk/news/politics/rss.xml',\n",
    "        'http://feeds.bbci.co.uk/news/health/rss.xml',\n",
    "        'http://feeds.bbci.co.uk/news/education/rss.xml',\n",
    "        'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml',\n",
    "        'http://feeds.bbci.co.uk/news/technology/rss.xml',\n",
    "        'http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml'\n",
    "         ]\n",
    "\n",
    "feeds=[]\n",
    "for url in RSS_URLS:\n",
    "    feeds.append(feedparser.parse(url))\n",
    "\n",
    "for feed in feeds:\n",
    "    for article in feed['entries']:\n",
    "        url=article['link']\n",
    "        time=article['updated']\n",
    "        title=article['title']\n",
    "        tag=feed.feed.title\n",
    "        content=request_content(article['link'])\n",
    "        save_to_csv(title,content,tag,time,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
